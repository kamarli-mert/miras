#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
OsmanlÄ±ca-TÃ¼rkÃ§e Veri Setlerini Ä°ÅŸleme Script'i
oe_tr.txt ve oe_tr.xlsx dosyalarÄ±nÄ± gÃ¶rsel eÄŸitim sistemine entegre eder
"""

import pandas as pd
import json
import os
from pathlib import Path
from typing import Dict, List, Tuple
import re

class OETRDataProcessor:
    """OsmanlÄ±ca-TÃ¼rkÃ§e veri setlerini iÅŸleyen sÄ±nÄ±f"""
    
    def __init__(self):
        self.data_dir = Path("..")  # Ana dizin
        self.output_dir = Path("data/training")
        self.character_mappings = self._load_character_mappings()
        
    def _load_character_mappings(self) -> Dict[str, str]:
        """Karakter eÅŸleÅŸmelerini yÃ¼kle"""
        return {
            'Ùƒ': 'k', 'Ú¯': 'g', 'Ù„': 'l', 'Ù…': 'm', 'Ù†': 'n', 'Ø±': 'r', 'Ø³': 's',
            'Øª': 't', 'Ø¨': 'b', 'ÛŒ': 'i', 'Ù‡': 'e', 'Ø¯': 'd', 'Ù': 'f', 'Ù‚': 'k',
            'Ø¹': '', 'Ø¤': 'Ã¼', 'Ø¡': '', 'Ø¦': '', 'Ø¢': 'a', 'Ø£': 'a', 'Ø¥': 'i',
            'Ø©': 'e', 'Ø´': 'ÅŸ', 'Ú†': 'Ã§', 'Ú˜': 'j', 'Ù¾': 'p', 'Øº': 'ÄŸ', 'Ø¸': 'z',
            'Ø·': 't', 'Ø¶': 'd', 'Øµ': 's', 'Ø«': 's', 'Ø®': 'h', 'Ø­': 'h', 'Ø°': 'z',
            'Ø²': 'z', 'Ùˆ': 'v', 'Ù‰': 'i', 'ï»»': 'la', 'ï·²': 'allah',
        }
    
    def process_txt_file(self, file_path: str) -> List[Dict]:
        """TXT dosyasÄ±nÄ± iÅŸle"""
        print(f"ğŸ“– TXT dosyasÄ± iÅŸleniyor: {file_path}")
        
        word_pairs = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if line and '\t' in line:
                        parts = line.split('\t')
                        if len(parts) >= 2:
                            ottoman = parts[0].strip()
                            turkish = parts[1].strip()
                            
                            if ottoman and turkish:
                                word_pairs.append({
                                    'ottoman': ottoman,
                                    'turkish': turkish,
                                    'source': 'txt',
                                    'line': line_num
                                })
            
            print(f"âœ… TXT dosyasÄ± iÅŸlendi: {len(word_pairs)} kelime Ã§ifti")
            return word_pairs
            
        except Exception as e:
            print(f"âŒ TXT dosyasÄ± iÅŸlenemedi: {e}")
            return []
    
    def process_xlsx_file(self, file_path: str) -> List[Dict]:
        """XLSX dosyasÄ±nÄ± iÅŸle"""
        print(f"ğŸ“Š XLSX dosyasÄ± iÅŸleniyor: {file_path}")
        
        word_pairs = []
        try:
            df = pd.read_excel(file_path)
            print(f"ğŸ“ˆ Excel dosyasÄ± yÃ¼klendi: {len(df)} satÄ±r")
            print(f"ğŸ“‹ SÃ¼tunlar: {list(df.columns)}")
            
            for index, row in df.iterrows():
                ottoman = str(row.get('OE', '')).strip()
                turkish = str(row.get('TR', '')).strip()
                
                if ottoman and turkish and ottoman != 'nan' and turkish != 'nan':
                    word_pairs.append({
                        'ottoman': ottoman,
                        'turkish': turkish,
                        'source': 'xlsx',
                        'line': index + 1
                    })
            
            print(f"âœ… XLSX dosyasÄ± iÅŸlendi: {len(word_pairs)} kelime Ã§ifti")
            return word_pairs
            
        except Exception as e:
            print(f"âŒ XLSX dosyasÄ± iÅŸlenemedi: {e}")
            return []
    
    def analyze_characters(self, word_pairs: List[Dict]) -> Dict:
        """Karakter analizi yap"""
        character_frequency = {}
        character_examples = {}
        
        for pair in word_pairs:
            ottoman_text = pair['ottoman']
            
            for char in ottoman_text:
                if char in self.character_mappings:
                    character_frequency[char] = character_frequency.get(char, 0) + 1
                    
                    if char not in character_examples:
                        character_examples[char] = []
                    
                    if len(character_examples[char]) < 10:  # Her karakter iÃ§in max 10 Ã¶rnek
                        character_examples[char].append({
                            'ottoman': ottoman_text,
                            'turkish': pair['turkish']
                        })
        
        return {
            'frequency': character_frequency,
            'examples': character_examples
        }
    
    def create_training_data(self, word_pairs: List[Dict], character_analysis: Dict):
        """EÄŸitim verileri oluÅŸtur"""
        
        # Ana Ã§eviri Ã§iftleri dosyasÄ±
        translation_file = self.output_dir / "translation_pairs.json"
        translation_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(translation_file, 'w', encoding='utf-8') as f:
            json.dump({
                'type': 'translation_pairs',
                'data': word_pairs,
                'count': len(word_pairs),
                'sources': list(set([pair['source'] for pair in word_pairs]))
            }, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Ã‡eviri Ã§iftleri kaydedildi: {translation_file}")
        print(f"ğŸ“Š Toplam {len(word_pairs)} Ã§eviri Ã§ifti")
        
        # Karakter analizi dosyasÄ±
        char_file = self.output_dir / "character_analysis.json"
        
        with open(char_file, 'w', encoding='utf-8') as f:
            json.dump({
                'type': 'character_analysis',
                'frequency': character_analysis['frequency'],
                'examples': character_analysis['examples'],
                'total_characters': len(character_analysis['frequency'])
            }, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Karakter analizi kaydedildi: {char_file}")
        print(f"ğŸ“Š Toplam {len(character_analysis['frequency'])} benzersiz karakter")
        
        # Her karakter iÃ§in Ã¶rnek dosyalarÄ± oluÅŸtur
        self.create_character_examples(character_analysis['examples'])
    
    def create_character_examples(self, character_examples: Dict):
        """Her karakter iÃ§in Ã¶rnek dosyalarÄ± oluÅŸtur"""
        
        for char, examples in character_examples.items():
            # Karakter adÄ±nÄ± belirle
            char_name = self.get_character_name(char)
            
            if char_name:
                char_dir = self.output_dir / char_name
                char_dir.mkdir(parents=True, exist_ok=True)
                
                # Karakter iÃ§in metin verisi dosyasÄ±
                text_file = char_dir / "text_data.json"
                
                with open(text_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        'character': char_name,
                        'ottoman_char': char,
                        'turkish_char': self.character_mappings.get(char, ''),
                        'examples': examples,
                        'count': len(examples)
                    }, f, ensure_ascii=False, indent=2)
                
                print(f"âœ… {char_name} iÃ§in metin verisi oluÅŸturuldu ({len(examples)} Ã¶rnek)")
    
    def get_character_name(self, char: str) -> str:
        """Karakter iÃ§in klasÃ¶r adÄ±nÄ± belirle"""
        char_mappings = {
            'Ø§': 'elif', 'Ø¨': 'be', 'Ù¾': 'pe', 'Øª': 'te', 'Ø«': 'se',
            'Ø¬': 'cim', 'Ø­': 'ha', 'Ø®': 'hi', 'Ø¯': 'dal', 'Ø°': 'zel',
            'Ø±': 're', 'Ø²': 'ze', 'Ø³': 'sin', 'Ø´': 'ÅŸin', 'Øµ': 'sad',
            'Ø¶': 'dad', 'Ø·': 'ta', 'Ø¸': 'za', 'Ø¹': 'ayn', 'Øº': 'gayn',
            'Ù': 'fe', 'Ù‚': 'kaf', 'Ùƒ': 'kef', 'Ù„': 'lam', 'Ù…': 'mim',
            'Ù†': 'nun', 'Ùˆ': 'vav', 'Ù‡': 'he', 'ÙŠ': 'ye'
        }
        
        return char_mappings.get(char, '')
    
    def generate_statistics(self, word_pairs: List[Dict], character_analysis: Dict):
        """Ä°statistikler oluÅŸtur"""
        
        print("\nğŸ“Š VERÄ° SETÄ° Ä°STATÄ°STÄ°KLERÄ°")
        print("=" * 50)
        
        # Genel istatistikler
        print(f"ğŸ“ Toplam kelime Ã§ifti: {len(word_pairs)}")
        print(f"ğŸ”¤ Benzersiz karakter: {len(character_analysis['frequency'])}")
        
        # Kaynak daÄŸÄ±lÄ±mÄ±
        sources = {}
        for pair in word_pairs:
            source = pair['source']
            sources[source] = sources.get(source, 0) + 1
        
        print(f"ğŸ“ Kaynak daÄŸÄ±lÄ±mÄ±:")
        for source, count in sources.items():
            print(f"   - {source}: {count} kelime Ã§ifti")
        
        # En Ã§ok kullanÄ±lan karakterler
        sorted_chars = sorted(character_analysis['frequency'].items(), 
                            key=lambda x: x[1], reverse=True)
        
        print(f"\nğŸ”¤ En Ã§ok kullanÄ±lan karakterler (Ä°lk 10):")
        for char, count in sorted_chars[:10]:
            char_name = self.get_character_name(char)
            print(f"   - {char} ({char_name}): {count} kez")
        
        # Ortalama kelime uzunluÄŸu
        ottoman_lengths = [len(pair['ottoman']) for pair in word_pairs]
        turkish_lengths = [len(pair['turkish']) for pair in word_pairs]
        
        avg_ottoman = sum(ottoman_lengths) / len(ottoman_lengths)
        avg_turkish = sum(turkish_lengths) / len(turkish_lengths)
        
        print(f"\nğŸ“ Ortalama kelime uzunluÄŸu:")
        print(f"   - OsmanlÄ±ca: {avg_ottoman:.1f} karakter")
        print(f"   - TÃ¼rkÃ§e: {avg_turkish:.1f} karakter")
    
    def run_processing(self):
        """TÃ¼m iÅŸleme sÃ¼recini Ã§alÄ±ÅŸtÄ±r"""
        print("ğŸš€ OsmanlÄ±ca-TÃ¼rkÃ§e Veri Ä°ÅŸleme BaÅŸlatÄ±lÄ±yor...")
        print("=" * 60)
        
        all_word_pairs = []
        
        # TXT dosyasÄ±nÄ± iÅŸle
        txt_file = self.data_dir / "oe_tr.txt"
        if txt_file.exists():
            txt_pairs = self.process_txt_file(str(txt_file))
            all_word_pairs.extend(txt_pairs)
        else:
            print(f"âš ï¸ TXT dosyasÄ± bulunamadÄ±: {txt_file}")
        
        # XLSX dosyasÄ±nÄ± iÅŸle
        xlsx_file = self.data_dir / "oe_tr.xlsx"
        if xlsx_file.exists():
            xlsx_pairs = self.process_xlsx_file(str(xlsx_file))
            all_word_pairs.extend(xlsx_pairs)
        else:
            print(f"âš ï¸ XLSX dosyasÄ± bulunamadÄ±: {xlsx_file}")
        
        if not all_word_pairs:
            print("âŒ HiÃ§ kelime Ã§ifti bulunamadÄ±!")
            return
        
        # Karakter analizi
        print("\nğŸ”¤ Karakter analizi yapÄ±lÄ±yor...")
        character_analysis = self.analyze_characters(all_word_pairs)
        
        # EÄŸitim verileri oluÅŸtur
        print("\nğŸ“ EÄŸitim verileri oluÅŸturuluyor...")
        self.create_training_data(all_word_pairs, character_analysis)
        
        # Ä°statistikler
        self.generate_statistics(all_word_pairs, character_analysis)
        
        print("\nâœ… Veri iÅŸleme tamamlandÄ±!")
        print("ğŸ¯ ArtÄ±k bu verileri gÃ¶rsel eÄŸitim sisteminde kullanabilirsiniz.")

def main():
    """Ana fonksiyon"""
    processor = OETRDataProcessor()
    processor.run_processing()

if __name__ == "__main__":
    main() 